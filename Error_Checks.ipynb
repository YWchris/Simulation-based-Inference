{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/home/tingli/.conda/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sbi.inference import SNPE\n",
    "from sbi import utils as utils\n",
    "from sbi.analysis import run_sbc, sbc_rank_plot\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import tarp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/raid/users/heigerm/catalogues/sp_x_apogee_x_spspectra_rvtab.fits\"\n",
    "# sp data\n",
    "HDUlist = fits.open(filename)\n",
    "# DESI\n",
    "sp_tab = Table(HDUlist['SPTAB'].data)   \n",
    "# APOGEE\n",
    "apogee_tab = Table(HDUlist['APOGEEDR17'].data) \n",
    "# DESI SP Spectra\n",
    "spectra = Table(HDUlist['SPECTRA_SP'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "targets = ['FE_H', 'MG_FE', 'C_FE', 'O_FE', 'CI_FE', 'AL_FE', 'SI_FE', 'K_FE', 'CA_FE', 'MN_FE', 'NI_FE', 'LOGG', 'TEFF']\n",
    "feh_target, mgfe_target, cfe_target, ofe_target, cife_target, alfe_target, sife_target, kfe_target, cafe_target, mnfe_target, nife_target, log_g, temperature = (np.array(apogee_tab[col]) for col in targets)\n",
    "targets_arr = [feh_target, kfe_target, cfe_target, cafe_target, nife_target, mnfe_target, ofe_target, cife_target, alfe_target]\n",
    "# check for Al error = 0 case\n",
    "alfe_target_err = np.array(apogee_tab['AL_FE_ERR'])\n",
    "abnormal_rows = np.unique([index for target in targets_arr for index, value in enumerate(target) if value > 10] + \n",
    "                          [index for index, value in enumerate(alfe_target_err) if value == 0])\n",
    "\n",
    "\n",
    "# Mask the abnormal rows across relevant datasets\n",
    "mask = ~np.isin(np.arange(len(apogee_tab)), abnormal_rows)\n",
    "apogee_tab_masked = apogee_tab[mask]\n",
    "spectra_masked = spectra[mask]\n",
    "sp_tab_masked = sp_tab[mask]\n",
    "\n",
    "# Reconstruct target arrays with masked data\n",
    "target_values_masked = {target: np.array(apogee_tab_masked[target]) for target in targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7336"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spectra_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine spectra from the three arms and normalize\n",
    "gb_combined_spectra = Table(names=['combined_flux', 'combined_wavelength'], dtype=['object', 'object'])\n",
    "\n",
    "for row in spectra_masked:\n",
    "    # Combine and sort flux and wavelength from all arms\n",
    "    combined_flux = np.concatenate([row['flx_B'], row['flx_R'], row['flx_Z']])\n",
    "    combined_wavelength = np.concatenate([row['B_WAVELENGTH'], row['R_WAVELENGTH'], row['Z_WAVELENGTH']])\n",
    "    sort_order = np.argsort(combined_wavelength)\n",
    "    combined_flux, combined_wavelength = combined_flux[sort_order], combined_wavelength[sort_order]\n",
    "\n",
    "    # Normalize flux\n",
    "    global_median = np.median(combined_flux)\n",
    "    IQR = np.percentile(combined_flux, 75) - np.percentile(combined_flux, 25)\n",
    "    normalized_flux = (combined_flux - global_median) / IQR\n",
    "\n",
    "    gb_combined_spectra.add_row([normalized_flux, combined_wavelength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = np.array(gb_combined_spectra['combined_flux'])\n",
    "# Input spectra\n",
    "X = np.array([np.array(flux_val, dtype=float) for flux_val in flux])\n",
    "# Parameters\n",
    "theta = np.column_stack([target_values_masked[target] for target in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 7336\n",
      "Dimensions of X: 13787\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of X:\", len(X))\n",
    "print(\"Dimensions of X:\", len(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "test_posterior_samples, sbc_ranks, sbc_dap_samples, all_x_test, all_y_test = [], [], [], [], []\n",
    "\n",
    "# Iterate through the folds\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, theta)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = theta[train_index], theta[test_index]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train, X_test, y_train, y_test = map(torch.Tensor, (X_train, X_test, y_train, y_test))\n",
    "\n",
    "    # Store test sets for later analysis\n",
    "    all_x_test.append(X_test)\n",
    "    all_y_test.append(y_test)\n",
    "\n",
    "    # load the posterior model saved\n",
    "    model_pkl_file = f\"SBI_fold_{fold}.pkl\" \n",
    "    \n",
    "    with open(model_pkl_file, 'rb') as file:\n",
    "        posterior = pickle.load(file)\n",
    "    \n",
    "    # Simulation-Based Calibration (SBC)\n",
    "    num_posterior_samples=1000\n",
    "    ranks, dap_samples = run_sbc(y_test, X_test, posterior, num_posterior_samples=num_posterior_samples, reduce_fns='marginals')\n",
    "    sbc_ranks.append(ranks)\n",
    "    sbc_dap_samples.append(dap_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checks - Simulation Based Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_x_test = torch.cat(all_x_test, dim = 0)\n",
    "sbc_x_test = sbc_x_test.numpy()\n",
    "\n",
    "sbc_y_test = torch.cat(all_y_test, dim = 0)\n",
    "sbc_y_test = sbc_y_test.numpy()\n",
    "\n",
    "sbc_ranks_test = torch.cat(sbc_ranks, dim = 0)\n",
    "sbc_ranks_test = sbc_ranks_test.numpy()\n",
    "\n",
    "sbc_dap_samples_test = torch.cat(sbc_dap_samples, dim = 0)\n",
    "sbc_dap_samples_test = sbc_dap_samples_test.numpy()\n",
    "\n",
    "sbc_ranks_test_tensor = torch.tensor(sbc_ranks_test)\n",
    "sbc_dap_samples_tensor = torch.tensor(sbc_dap_samples_test)\n",
    "sbc_y_test_tensor = torch.tensor(sbc_y_test)\n",
    "\n",
    "# KS test\n",
    "#check_stats = check_sbc(sbc_ranks_test_tensor, sbc_y_test, sbc_dap_samples_tensor, num_posterior_samples=num_posterior_samples)\n",
    "#print(f\"kolmogorov-smirnov p-values \\ncheck_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}\")\n",
    "\n",
    "# SBC Rank Plot\n",
    "f, ax = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    parameter_labels = ['Fe/H', 'Mg/Fe', 'O/Fe', 'C/Fe', 'CI/Fe', 'Si/Fe', 'K/Fe', 'Ca/Fe', 'Al/Fe', 'Mn/Fe', 'Ni/Fe', \n",
    "                        'Log_g', 'Teff'],\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=None)\n",
    "\n",
    "# SBC CDF Plot\n",
    "f, ax = sbc_rank_plot(ranks, 1_000, parameter_labels = ['Fe/H', 'Mg/Fe', 'O/Fe', 'C/Fe', 'CI/Fe', 'Si/Fe', 'K/Fe', 'Ca/Fe', 'Al/Fe', 'Mn/Fe', 'Ni/Fe',\n",
    "                                                       'Log_g', 'Teff'], \n",
    "                      plot_type=\"cdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 13\n",
    "n_samples = 250\n",
    "\n",
    "post_samples = torch.cat(test_posterior_samples, dim = 0)\n",
    "post_samples = post_samples.numpy()\n",
    "posterior_samples = post_samples.reshape((n_samples, len(sbc_x_test), n_dims))\n",
    "\n",
    "coverage_values, ecp = tarp.get_drp_coverage(posterior_samples, sbc_y_test, references='random', metric='euclidean')\n",
    "\n",
    "plt.plot(coverage_values, ecp, marker='o')\n",
    "plt.xlabel(\"Credibility Level\")\n",
    "plt.ylabel(\"Expected Coverage Probability\")\n",
    "plt.title(\"Credibility Level vs. Expected Coverage Probability\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "# Step 2: Compare each ECP value with the significance level\n",
    "is_significant = ecp <= significance_level\n",
    "\n",
    "# Step 3: Calculate the p-value\n",
    "p_value = is_significant.mean()\n",
    "\n",
    "print(\"p-value:\", p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3c0d2cb689c9f3ddfdcc20370a54f5a0d1f4658107fa1312f8e0c21d7f27d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
