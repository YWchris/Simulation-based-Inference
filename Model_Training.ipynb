{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/home/tingli/.conda/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sbi.inference import SNPE\n",
    "from sbi import utils as utils\n",
    "from sbi.analysis import run_sbc, sbc_rank_plot\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/raid/users/heigerm/catalogues/sp_x_apogee_x_spspectra_rvtab.fits\"\n",
    "# sp data\n",
    "HDUlist = fits.open(filename)\n",
    "# DESI\n",
    "sp_tab = Table(HDUlist['SPTAB'].data)   \n",
    "# APOGEE\n",
    "apogee_tab = Table(HDUlist['APOGEEDR17'].data) \n",
    "# DESI SP Spectra\n",
    "spectra = Table(HDUlist['SPECTRA_SP'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "targets = ['FE_H', 'MG_FE', 'C_FE', 'O_FE', 'CI_FE', 'AL_FE', 'SI_FE', 'K_FE', 'CA_FE', 'MN_FE', 'NI_FE', 'LOGG', 'TEFF']\n",
    "feh_target, mgfe_target, cfe_target, ofe_target, cife_target, alfe_target, sife_target, kfe_target, cafe_target, mnfe_target, nife_target, log_g, temperature = (np.array(apogee_tab[col]) for col in targets)\n",
    "targets_arr = [feh_target, kfe_target, cfe_target, cafe_target, nife_target, mnfe_target, ofe_target, cife_target, alfe_target]\n",
    "# check for Al error = 0 case\n",
    "alfe_target_err = np.array(apogee_tab['AL_FE_ERR'])\n",
    "abnormal_rows = np.unique([index for target in targets_arr for index, value in enumerate(target) if value > 10] + \n",
    "                          [index for index, value in enumerate(alfe_target_err) if value == 0])\n",
    "\n",
    "\n",
    "# Mask the abnormal rows across relevant datasets\n",
    "mask = ~np.isin(np.arange(len(apogee_tab)), abnormal_rows)\n",
    "apogee_tab_masked = apogee_tab[mask]\n",
    "spectra_masked = spectra[mask]\n",
    "sp_tab_masked = sp_tab[mask]\n",
    "\n",
    "# Reconstruct target arrays with masked data\n",
    "target_values_masked = {target: np.array(apogee_tab_masked[target]) for target in targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7336"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spectra_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine spectra from the three arms and normalize\n",
    "gb_combined_spectra = Table(names=['combined_flux', 'combined_wavelength'], dtype=['object', 'object'])\n",
    "\n",
    "for row in spectra_masked:\n",
    "    # Combine and sort flux and wavelength from all arms\n",
    "    combined_flux = np.concatenate([row['flx_B'], row['flx_R'], row['flx_Z']])\n",
    "    combined_wavelength = np.concatenate([row['B_WAVELENGTH'], row['R_WAVELENGTH'], row['Z_WAVELENGTH']])\n",
    "    sort_order = np.argsort(combined_wavelength)\n",
    "    combined_flux, combined_wavelength = combined_flux[sort_order], combined_wavelength[sort_order]\n",
    "\n",
    "    # Normalize flux\n",
    "    global_median = np.median(combined_flux)\n",
    "    IQR = np.percentile(combined_flux, 75) - np.percentile(combined_flux, 25)\n",
    "    normalized_flux = (combined_flux - global_median) / IQR\n",
    "\n",
    "    gb_combined_spectra.add_row([normalized_flux, combined_wavelength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = np.array(gb_combined_spectra['combined_flux'])\n",
    "# Input spectra\n",
    "X = np.array([np.array(flux_val, dtype=float) for flux_val in flux])\n",
    "# Parameters\n",
    "theta = np.column_stack([target_values_masked[target] for target in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 7336\n",
      "Dimensions of X: 13787\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of X:\", len(X))\n",
    "print(\"Dimensions of X:\", len(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 31 60 epochs."
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the neural network structure for embedding\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(13787, 4000),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4000, 2000),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2000, 1000),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(500, 100),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(100, 50))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Iterate through the folds\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, theta)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = theta[train_index], theta[test_index]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train, X_test, y_train, y_test = map(torch.Tensor, (X_train, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "    # Initialize the neural posterior with the defined model\n",
    "    neural_posterior = utils.posterior_nn(model=\"nsf\", embedding_net=Model(), hidden_features=50, num_transforms=5)\n",
    "    inference = SNPE(density_estimator=neural_posterior)\n",
    "    inference.append_simulations(y_train, X_train)\n",
    "\n",
    "    # Train the density estimator and build posterior\n",
    "    density_estimator = inference.train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "            \n",
    "    # Save the model\n",
    "    model_pkl_file = f\"SBI_fold_{fold}.pkl\" \n",
    "\n",
    "    with open(model_pkl_file, 'wb') as file:  \n",
    "        pickle.dump(posterior, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3c0d2cb689c9f3ddfdcc20370a54f5a0d1f4658107fa1312f8e0c21d7f27d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
